@inproceedings{Ding_SSII2024,
  author    = {丁 寧 and 武田 一哉 and Jin Wenhui and Bei Yingjiu and 藤井 慶輔},
  booktitle = {SSII2024},
  title     = {多視点ドローン映像からの姿勢情報を用いたバドミントンダブルスの最適ポジショニングの推定},
  year      = {2024},
  yomi      = {Ning Ding and K Takeda and Jin Wenhui and Bei Yingjiu and Y Fujii}
}

@inproceedings{Fukuzawa_MMM2025_Zero_shot,
  address   = {Singapore},
  author    = {Fukuzawa, Takumi
               and Hara, Kensho
               and Kataoka, Hirokatsu
               and Tamaki, Toru},
  booktitle = {MultiMedia Modeling},
  editor    = {Ide, Ichiro
               and Kompatsiaris, Ioannis
               and Xu, Changsheng
               and Yanai, Keiji
               and Chu, Wei-Ta
               and Nitta, Naoko
               and Riegler, Michael
               and Yamasaki, Toshihiko},
  isbn      = {978-981-96-2071-5},
  pages     = {366--379},
  publisher = {Springer Nature Singapore},
  title     = {Can Masking Background and Object Reduce Static Bias for Zero-Shot Action Recognition?},
  year      = {2025}
}

@inproceedings{Fukuzawa_SSII2024_Zero_shot,
  author    = {福沢 匠 and 原 健翔 and 片岡 裕雄 and 玉木 徹},
  booktitle = {SSII2024},
  title     = {静的バイアスの強弱に関わらないZero-shot動作認識手法の検討},
  year      = {2024},
  yomi      = {Takumi Fukuzawa and Kensho Hara and Hirokatsu Kataoka and Toru Tamaki}
}

@inproceedings{Hashiguchi_2022_ACCVW_MSCA,
  author    = {Hashiguchi, Ryota and Tamaki, Toru},
  booktitle = {Proceedings of the Asian Conference on Computer Vision (ACCV) Workshops},
  month     = {December},
  pages     = {276-288},
  title     = {Temporal Cross-attention for Action Recognition},
  url       = {https://openaccess.thecvf.com/content/ACCV2022W/TCV/html/Hashiguchi_Temporal_Cross-attention_for_Action_Recognition_ACCVW_2022_paper.html},
  year      = {2022}
}

@article{Hashiguchi_arxiv2022_MSCA,
  author     = {Ryota Hashiguchi and
                Toru Tamaki},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2204-00452.bib},
  doi        = {10.48550/arXiv.2204.00452},
  eprint     = {2204.00452},
  eprinttype = {arXiv},
  journal    = {CoRR},
  timestamp  = {Wed, 06 Apr 2022 14:29:31 +0200},
  title      = {Vision Transformer with Cross-attention by Temporal Shift for Efficient
                Action Recognition},
  url        = {https://doi.org/10.48550/arXiv.2204.00452},
  volume     = {abs/2204.00452},
  year       = {2022}
}

@inproceedings{Hashiguchi_MIRU2024_TAL,
  author    = {橋口 凌大 and 玉木 徹},
  booktitle = {MIRU2024},
  title     = {動作区間検出データセットの動作区間長に応じたノイズ除去},
  year      = {2024},
  yomi      = {Ryota Hasigichi and Toru Tamaki}
}

@misc{Hori_arXiv2024_STAD_query_matching,
  archiveprefix = {arXiv},
  author        = {Shimon Hori and Kazuki Omi and Toru Tamaki},
  eprint        = {2409.18408},
  primaryclass  = {cs.CV},
  title         = {Query matching for spatio-temporal action detection with query-based object detector},
  url           = {https://arxiv.org/abs/2409.18408},
  year          = {2024}
}
@inproceedings{Hori_SSII2024_STAD_query_matching,
  author    = {堀 史門 and 大見 一樹 and 玉木 徹},
  booktitle = {SSII2024},
  title     = {フレーム間のクエリマッチングを用いた物体検出モデルの時空間動作検出への拡張},
  year      = {2024},
  yomi      = {Shimon Hori and Kazuki Omi and Toru Tamaki}
}

@inproceedings{Kamiya_IWFCV2024_Multi_Model,
  author       = {Kodai Kamiya and Toru Tamaki},
  booktitle    = {The 30th International Workshop on Frontiers of Computer Vision (IW-FCV2024)},
  organization = {The Institute of Electrical Engineers of Japan},
  title        = {Multi-model learning by sequential reading of untrimmed videos for action recognition},
  url          = {https://www.scitepress.org/Link.aspx?doi=10.5220/0012310400003660},
  url          = {https://arxiv.org/abs/2401.14675},
  year         = {2024}
}

@inproceedings{Kato_GCCE2024_online_video,
  author    = {Kato, Itsuki and Kamiya, Kodai and Tamaki, Toru},
  booktitle = {2024 IEEE 13th Global Conference on Consumer Electronics (GCCE)},
  doi       = {10.1109/GCCE62371.2024.10760638},
  keywords  = {Image recognition;Correlation;Contrastive learning;Consumer electronics;untrimmed video;pre-training;action recognition},
  number    = {},
  pages     = {462-463},
  title     = {Online pre-training with long-form videos},
  volume    = {},
  year      = {2024}
}

@inproceedings{Kato_SSII2024_online_video,
  author    = {加藤 樹 and 玉木 徹},
  booktitle = {SSII2024},
  title     = {長時間動画のオンライン事前学習方法の検討},
  year      = {2024},
  yomi      = {Itsuki Kato and Toru Tamaki}
}

@inproceedings{Kimata_MIRU2024_shared_LoRA,
  author    = {木全 潤 and 玉木 徹},
  booktitle = {MIRU2024},
  title     = {動作認識における複数データセット学習のための固有LoRAと共有LoRA},
  year      = {2024},
  yomi      = {Jun Kimata and Toru Tamaki}
}

@inproceedings{Kimata_MMAsia2022_ObjectMix,
  address   = {New York, NY, USA},
  articleno = {26},
  author    = {Kimata, Jun and Nitta, Tomoya and Tamaki, Toru},
  booktitle = {Proceedings of the 4th ACM International Conference on Multimedia in Asia},
  doi       = {10.1145/3551626.3564941},
  isbn      = {9781450394789},
  keywords  = {action recognition, instance segmentation, data augmentation},
  location  = {Tokyo, Japan},
  numpages  = {7},
  publisher = {Association for Computing Machinery},
  series    = {MMAsia '22},
  title     = {ObjectMix: Data Augmentation by Copy-Pasting Objects in Videos for Action Recognition},
  url       = {https://doi.org/10.1145/3551626.3564941},
  year      = {2022}
}

@misc{Mizuno_arXiv2024_VideoSegmentation,
  archiveprefix = {arXiv},
  author        = {Tsubasa Mizuno and Toru Tamaki},
  eprint        = {2410.07635},
  primaryclass  = {cs.CV},
  title         = {Shift and matching queries for video semantic segmentation},
  url           = {https://arxiv.org/abs/2410.07635},
  year          = {2024}
}

@inproceedings{Mizuno_SSII2024_VideoSegmentation,
  author    = {水野 翼 and 玉木 徹},
  booktitle = {SSII2024},
  title     = {画像セマンティックセグメンテーションの動画像への効率的な拡張},
  year      = {2024},
  yomi      = {Tsubasa Mizuno and Toru Tamaki}
}

@article{Nitta_IEEE_Access_2024_captioning,
  author  = {Nitta, Tomoya and Fukuzawa, Takumi and Tamaki, Toru},
  doi     = {10.1109/ACCESS.2024.3506751},
  journal = {IEEE Access},
  number  = {},
  pages   = {1-1},
  title   = {Fine-grained length controllable video captioning with ordinal embeddings},
  volume  = {},
  year    = {2024}
}

@article{Nitta_IEICEED_2023_Object-ABN,
  author  = {Tomoya Nitta and
             Tsubasa Hirakawa and
             Hironobu Fujiyoshi and
             Toru Tamaki},
  doi     = {10.1587/transinf.2022EDP7138},
  journal = {IEICE Transactions on Information and Systems},
  number  = {3},
  pages   = {391-400},
  title   = {Object-ABN: Learning to Generate Sharp Attention Maps for Action Recognition},
  url     = {https://doi.org/10.1587/transinf.2022EDP7138},
  volume  = {E106.D},
  year    = {2023}
}

@inproceedings{Nitta_MIRU2024_Length_Embedding,
  author    = {仁田 智也 and 玉木 徹},
  booktitle = {MIRU2024},
  title     = {動画の説明文生成における長さ制御のための埋め込みとその解析},
  year      = {2024},
  yomi      = {Tomoya Nitta and Toru Tamaki}
}

@article{Omi_IEICE-ED2022_MDL,
  author  = {Kazuki Omi and Jun Kimata and Toru Tamaki},
  doi     = {10.1587/transinf.2022EDP7058},
  journal = {IEICE Transactions on Information and Systems},
  number  = {12},
  pages   = {2119-2126},
  title   = {Model-Agnostic Multi-Domain Learning with Domain-Specific Adapters for Action Recognition},
  url     = {https://doi.org/10.1587/transinf.2022EDP7058},
  volume  = {E105-D},
  year    = {2022}
}

@inproceedings{Omi_IWAIT2022_ADDA,
  author    = {Kazuki Omi and Toru Tamaki},
  booktitle = {International Workshop on Advanced Imaging Technology (IWAIT) 2022},
  doi       = {10.1117/12.2625953},
  month     = {January},
  title     = {On the instability of unsupervised domain adaptation with ADDA},
  url       = {https://doi.org/10.1117/12.2625953},
  volume    = {121771X},
  year      = {2021}
}

@inproceedings{Omi_MIRU2024_ActionTube,
  author    = {大見 一樹 and 玉木 徹},
  booktitle = {MIRU2024},
  title     = {時空間アクション検出のための人物クエリのマッチングによるアクションチューブ生成},
  year      = {2024},
  yomi      = {Kazuki Omi and Toru Tamaki}
}

@inproceedings{Omi_VISAPP2025_ActionTube,
  author    = {Kazuki Omi and Jion Oshima and Toru Tamaki},
  booktitle = {International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications (VISAPP2025)},
  title     = {Action tube generation by person query matching for spatio-temporal action detection},
  year      = {2025}
}

@inproceedings{Oshima_MIRU2024_AVA_Query_Matching,
  author    = {大島 慈温 and 堀 史門 and 玉木 徹},
  booktitle = {MIRU2024},
  title     = {時空間動作検出のための人物クエリマッチングのAVAデータセットに対する性能評価},
  year      = {2024},
  yomi      = {Jion Oshima and Shimon Hori and Toru Tamaki}
}

@article{Otani_IEEEAccess2022_MPEG_JPEG,
  author    = {Aoi Otani and
               Ryota Hashiguchi and
               Kazuki Omi and
               Norishige Fukushima and
               Toru Tamaki},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/journals/access/OtaniHOFT22.bib},
  doi       = {10.1109/ACCESS.2022.3204755},
  journal   = {{IEEE} Access},
  pages     = {94898--94907},
  timestamp = {Sun, 02 Oct 2022 15:26:28 +0200},
  title     = {Performance Evaluation of Action Recognition Models on Low Quality
               Videos},
  url       = {https://doi.org/10.1109/ACCESS.2022.3204755},
  volume    = {10},
  year      = {2022}
}

@inproceedings{Shimizu_MVA2023_IV_ViT,
  author    = {Shimizu, Shuki and Tamaki, Toru},
  booktitle = {2023 18th International Conference on Machine Vision and Applications (MVA)},
  doi       = {10.23919/MVA57639.2023.10215661},
  keywords  = {Training;Image recognition;Machine vision;Transformers;Tuning;Videos},
  number    = {},
  pages     = {1-6},
  title     = {Joint learning of images and videos with a single Vision Transformer},
  url       = {https://ieeexplore.ieee.org/document/10215661},
  volume    = {},
  year      = {2023}
}

@inproceedings{Sugiura_VISAPP2024_S3Aug,
  author       = {Taiki Sugiura and Toru Tamaki},
  booktitle    = {Proceedings of the 19th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications - Volume 2: VISAPP},
  doi          = {10.5220/0012310400003660},
  isbn         = {978-989-758-679-8},
  issn         = {2184-4321},
  organization = {INSTICC},
  pages        = {71-79},
  publisher    = {SciTePress},
  title        = {S3Aug: Segmentation, Sampling, and Shift for Action Recognition},
  url          = {https://www.scitepress.org/Link.aspx?doi=10.5220/0012310400003660},
  year         = {2024}
}
