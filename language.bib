@inproceedings{Devlin-ACL2019-BERT,
  abstract  = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  address   = {Minneapolis, Minnesota},
  author    = {Devlin, Jacob  and
               Chang, Ming-Wei  and
               Lee, Kenton  and
               Toutanova, Kristina},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  doi       = {10.18653/v1/N19-1423},
  month     = jun,
  pages     = {4171--4186},
  publisher = {Association for Computational Linguistics},
  title     = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  url       = {https://aclanthology.org/N19-1423},
  year      = {2019}
}

@misc{Radford_2019_GPT2,
  author       = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  howpublished = {\url{https://openai.com/blog/better-language-models/}},
  month        = {Feb},
  note         = {OpenAI blog},
  title        = {Language Models are Unsupervised Multitask Learners},
  year         = {2019}
}

@article{Sanh_arXiv2019_DistilBERT,
  author     = {Victor Sanh and
                Lysandre Debut and
                Julien Chaumond and
                Thomas Wolf},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1910-01108.bib},
  eprint     = {1910.01108},
  eprinttype = {arXiv},
  journal    = {CoRR},
  timestamp  = {Tue, 02 Jun 2020 12:48:59 +0200},
  title      = {DistilBERT, a distilled version of {BERT:} smaller, faster, cheaper
                and lighter},
  url        = {http://arxiv.org/abs/1910.01108},
  volume     = {abs/1910.01108},
  year       = {2019}
}